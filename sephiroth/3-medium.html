<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Sampling the Imaginary | Rethinking Bayes</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Sampling the Imaginary" />
<meta name="author" content="Philip Griffith" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Medium" />
<meta property="og:description" content="Medium" />
<link rel="canonical" href="https://philipgriffith.github.io/RethinkingBayes/sephiroth/3-medium" />
<meta property="og:url" content="https://philipgriffith.github.io/RethinkingBayes/sephiroth/3-medium" />
<meta property="og:site_name" content="Rethinking Bayes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2000-12-24T00:00:00-05:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"https://philipgriffith.github.io/RethinkingBayes/sephiroth/3-medium","headline":"Sampling the Imaginary","dateModified":"2000-12-24T00:00:00-05:00","datePublished":"2000-12-24T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://philipgriffith.github.io/RethinkingBayes/sephiroth/3-medium"},"author":{"@type":"Person","name":"Philip Griffith"},"description":"Medium","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/RethinkingBayes/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/RethinkingBayes/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/RethinkingBayes/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/RethinkingBayes/assets/apple-touch-icon.png">
  
  <!-- MathJax -->
  

  <!-- Google Analytics-->
  
</head>

<body>

	<nav class="nav">
  <div class="nav-container">
    <a href="/RethinkingBayes/">
      <h2 class="nav-title">Rethinking Bayes</h2>
    </a>
    <ul>
      <li><a href="/RethinkingBayes/">Chapters</a></li>
	  <li><a href="/RethinkingBayes/resources">Resources</a></li>
	  <li><a href="/RethinkingBayes/about">About</a></li>
    </ul>
  </div>
</nav>


	<main>
		<div class="post">

	

	<h1 class="post-title">3. Sampling the Imaginary</h1>
	<div class="post-line"></div>

	<h1 id="medium">Medium</h1>

<p>Weâ€™ll use this helper function to visualize the results of the posterior probability checks:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">vline</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">xvar</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">value</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">axvspan</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">value</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">vline</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">vline</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#d55e00'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Actual # of Boys'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">f'# of </span><span class="si">{</span><span class="n">xvar</span><span class="si">}</span><span class="s"> Observations'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Frequency'</span><span class="p">)</span></code></pre></figure>

<hr />

<p><strong>3M1.</strong> Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">var</span> <span class="o">=</span> <span class="s">'Water'</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">pg</span><span class="p">,</span> <span class="n">po</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">compute_grid_approximation</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">success</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">tosses</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">po</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">f'Probability of </span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Density'</span><span class="p">)</span></code></pre></figure>

<p><img src="/RethinkingBayes/assets/images/3m1.png" alt="3M1" title="3M1" /></p>

<hr />

<p><strong>3M2.</strong> Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">po</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">hdi</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">plot_interval</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">right</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xvar</span><span class="o">=</span><span class="n">var</span><span class="p">)</span></code></pre></figure>

<p><strong><center>The 90% HDPI for p = [0.329, 0.712]</center></strong></p>

<p><img src="/RethinkingBayes/assets/images/3m2.png" alt="3M2" title="3M2" /></p>

<hr />

<p><strong>3M3.</strong> Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ppc</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ppc</span> <span class="o">==</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">xvar</span><span class="o">=</span><span class="n">var</span><span class="p">)</span></code></pre></figure>

<p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html" target="_blank"><code class="highlighter-rouge">Documentation for stats.binom.rvs</code></a></p>

<p><strong><center>p = 0.144</center></strong></p>

<p><img src="/RethinkingBayes/assets/images/3m3.png" alt="3M3" title="3M3" /></p>

<hr />

<p><strong>3M4.</strong> Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ppc</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ppc</span> <span class="o">==</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">xvar</span><span class="o">=</span><span class="n">var</span><span class="p">)</span></code></pre></figure>

<p><a href="https://numpy.org/doc/stable/reference/generated/numpy.percentile.html" target="_blank"><code class="highlighter-rouge">Documentation for stats.binom.rvs</code></a></p>

<p><strong><center>p = 0.177</center></strong></p>

<p><img src="/RethinkingBayes/assets/images/3m4.png" alt="3M4" title="3M4" /></p>

<hr />

<p><strong>3M5.</strong> Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earthâ€™s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pg</span><span class="p">,</span> <span class="n">po</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">compute_grid_approximation</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">success</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">tosses</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">po</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">f'Probability of </span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Density'</span><span class="p">)</span></code></pre></figure>

<p><img src="/RethinkingBayes/assets/images/3m5a.png" alt="3M5a" title="3M5a" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">po</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">hdi</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">plot_interval</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">right</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xvar</span><span class="o">=</span><span class="n">var</span><span class="p">)</span></code></pre></figure>

<p><strong><center>The 90% HDPI for p = [0.501, 0.711]</center></strong></p>

<p>Because the new prior assumes that values below 0.5 are impossible, the left interval now begins at that value. Despite this prior, the greatest density of the posterior distribution still falls below a value of approximately 70%, given the data.</p>

<p><img src="/RethinkingBayes/assets/images/3m5b.png" alt="3M5b" title="3M5b" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ppc</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ppc</span> <span class="o">==</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">xvar</span><span class="o">=</span><span class="n">var</span><span class="p">)</span></code></pre></figure>

<p><strong><center>p = 0.156</center></strong></p>

<p>Because the new prior gives higher probabilities to values above 0.5, the probability of observing eight waters in fifteen tosses has grown, though only very slightly. More noticeable is that observing more than or fewer than eight waters has grown more and less probable, respectively.</p>

<p><img src="/RethinkingBayes/assets/images/3m5c.png" alt="3M5c" title="3M5c" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ppc</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ppc</span> <span class="o">==</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">xvar</span><span class="o">=</span><span class="n">var</span><span class="p">)</span></code></pre></figure>

<p><strong><center>p = 0.231</center></strong></p>

<p>The same probabilistic shift towards higher values influences this inference, but, because there is less data than before, the prior plays a larger role in determining the shape of the posterior, and therefore the probability of observing six waters in nine tosses shows a more pronounced increase.</p>

<p><img src="/RethinkingBayes/assets/images/3m5d.png" alt="3M5d" title="3M5d" /></p>


</div>

<div class="pagination">
	
	<a href="/RethinkingBayes/sephiroth/3-easy" class="left arrow">&#8592;</a>
	
	
	<a href="/RethinkingBayes/sephiroth/3-hard" class="right arrow">&#8594;</a>
	
	<a href="#" class="top">Top</a>
</div>
	</main>

	<footer>
  <span>
    &copy; <time datetime="2020-07-20 22:39:38 -0400">2020</time> Philip Griffith. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/" target="_blank">Tale</a> theme.
  </span>
</footer>

	
</body>
</html>